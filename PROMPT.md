ok, currently we are using transcribe-translate pipeline. problem is for korean and english direction is vise versa. so while i am speaking and not finishing sentence, and trying to translate in the middle, the translation is often very wrong. so in order to improve, i suggest the following improvlement. we will keep the current one as is. we will introduce new config. it only works under transcribe-trnaslate pipeline. new config will toggle between current behavior and new behavior. the new behavior description is following. we will transcribe as fast. 1000 is good. we will have two windows horizontally positioned. when new transcription comes, we will display in the left box. and then we will have to figure out the transcript contains the full sentence.  most likely they are not going to be a full sentence. we will have to send to LLM to check if it has full sentence. (checking chronological order). display full sentences on the transcript box. and cut the rest (not sentence) into the buffer. then when next transcript comes, add that into the buffer and do the same process of sending it to LLM and separate out full sentense and non-finished letters. so after LLM sentence determination is done, display the transcriptions, but make new line for the complete sentences, and just add inline for non complete letters. then send completed sentences to translate LLM model for full sentence translation, then show that to the right translation box. also make toggle ui to visually display or hide the transcription box. 


I wanna add a new feature where you can configure. its "proof reading" it only works transcribe-translate mode with sentenceBuffered is on. when proofreading is on, here is the flow. user speaks, recording goes to the transcription model and we get the transcribed text. we prefix leftout buffers and use sentenceDetector to detect sentences. for the detected sentences, we keep the upto x (default 20) sentences into the contextBuffer memory, alone with the existing sentences and newly detected sentences, send them into proof reader asking if we there is miss transcription on the new sentences and based on previous 20 sentences on top of it, and then ask it to correct the words if some words are out of the making sense. (to mitigate transcription model quality issue). then we push the newly added sentences into the contextBuffer (again up to 20 sentences) then send the newly detected and possibly corrected sentences into the translator.



i feel like i still have room for improvement. i still have issue determining sentences out of the transcription. when transcription is arrived, we determine sentences and pending. but sometimes the pending is meant to be actually end of the sentence, sometimes pending bit should be intended to be continued but LLM determine it as the sentence. it happens more often when recording interval is shorter. in order to mitigate this, i want to shorten the interval: 500. then send it to sentence detector. but after response comes back, instead of proofread => translate imediately, wait for the next one and and concat with the previous transcription then send two versions for sentence detection, and determine either first one or second one makes more sense. if first one made more sense, then we are good to send to proofreader=>translator, if second one made more sense, then wait for the next transcript and repeat. following this logic, if second one is empty. and if both first one and second one is empty (no voice for two intervals), then any pending buffers gets flushed and treated as sentence and sent over to the next process.
